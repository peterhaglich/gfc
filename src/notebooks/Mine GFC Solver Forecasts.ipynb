{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/peterhaglich/Dropbox/Work/IARPA/HFC/gfc/gfc/src/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/peterhaglich/Dropbox/Work/IARPA/HFC/gfc/gfc/src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from gfcapi.gfcapi import GfcApi, GfcAdminApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0a4d8f1a34501875101650ee2d60a2dc9dd27c38f0e1262a57ce7e1af1743a1\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\"..\", \"data\", \"gfc2_data\")\n",
    "AUTH_PATH = os.path.join(\"resources\", \"auth.json\")\n",
    "with open(AUTH_PATH, \"r\") as f:\n",
    "    auth_dict = json.load(f)\n",
    "    secret_token = auth_dict['gfc_token']\n",
    "    print(secret_token)\n",
    "SERVER = 'https://api.iarpagfchallenge.com'\n",
    "Q_ENDPOINT = '/api/v1/questions'\n",
    "url = SERVER + '/api/v1/questions' # The endpoint to retrieve questions\n",
    "headers = {'Authorization':'Bearer ' + secret_token}\n",
    "params = {} # More to come on this in a moment\n",
    "instance='production'\n",
    "gf=GfcAdminApi(token=secret_token,server=SERVER,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DAY_0 = \"2019-05-15\"\n",
    "DAY_N = \"2019-07-08\"\n",
    "\n",
    "\n",
    "rct_dr = pd.date_range(DAY_0, DAY_N)\n",
    "the_days = [x.date().isoformat() for x in rct_dr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the consensus histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2019-05-15\n",
      "There are 300 pages\n",
      "Processing 2019-05-16\n",
      "There are 1439 pages\n",
      "Processing 2019-05-17\n",
      "There are 1148 pages\n",
      "Processing 2019-05-18\n",
      "There are 965 pages\n",
      "Processing 2019-05-19\n",
      "There are 1096 pages\n",
      "Processing 2019-05-20\n",
      "There are 1268 pages\n",
      "Processing 2019-05-21\n",
      "There are 1347 pages\n",
      "Processing 2019-05-22\n",
      "There are 1438 pages\n",
      "Processing 2019-05-23\n",
      "There are 2486 pages\n",
      "Processing 2019-05-24\n",
      "There are 2037 pages\n",
      "Processing 2019-05-25\n",
      "There are 1979 pages\n",
      "Processing 2019-05-26\n",
      "There are 1937 pages\n",
      "Processing 2019-05-27\n",
      "There are 2407 pages\n",
      "Processing 2019-05-28\n",
      "There are 2176 pages\n",
      "Processing 2019-05-29\n",
      "There are 2655 pages\n",
      "Processing 2019-05-30\n",
      "There are 3825 pages\n",
      "Processing 2019-05-31\n",
      "There are 3694 pages\n",
      "Processing 2019-06-01\n",
      "There are 3257 pages\n",
      "Processing 2019-06-02\n",
      "There are 3585 pages\n",
      "Processing 2019-06-03\n",
      "There are 3494 pages\n"
     ]
    }
   ],
   "source": [
    "for dd in the_days:\n",
    "    print(\"Processing {}\".format(dd))\n",
    "    _created_after = \"{}T00:00:00\".format(dd)\n",
    "    _created_before = \"{}T23:59:59\".format(dd)\n",
    "    params = {\"created_after\": parse(_created_after),\n",
    "              \"created_before\": parse(_created_before)}\n",
    "    _cons = gf.get_external_prediction_sets(**params)\n",
    "    _path = os.path.join(DATA_PATH, \"External_Prediction_Sets\",\"External_Prediction_Sets_{}.json\".format(dd))\n",
    "    with open(_path, \"w\") as f:\n",
    "        json.dump(_cons, f, ensure_ascii=False, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the individual human forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dd in the_days:\n",
    "    print(\"Processing {}\".format(dd))\n",
    "    _created_after = \"{}T00:00:00\".format(dd)\n",
    "    _created_before = \"{}T23:59:59\".format(dd)\n",
    "    params = {\"created_after\": parse(_created_after),\n",
    "              \"created_before\": parse(_created_before)}\n",
    "    _cons = gf.get_human_forecasts(training_data=False, **params)\n",
    "    _path = os.path.join(DATA_PATH, \"Human_Forecasts\",\"Human_Forecasts_{}.json\".format(dd))\n",
    "    with open(_path, \"w\") as f:\n",
    "        json.dump(_cons, f, ensure_ascii=False, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
